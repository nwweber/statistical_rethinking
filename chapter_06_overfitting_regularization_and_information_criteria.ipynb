{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress futurewarnings caused by pandas and other libraries not updating their numpy interface codes\n",
    "# apparently has to come before these import, so leave this on top\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Overfitting, Regularization, and Information Criteria\n",
    "\n",
    "**Ockham's Razor** making aesthetic/pragmatic preferenc for simpler model more explicit: prefer models w/ fewer assumptions. For same performance, choose simpler model. But what to do for different trade-offs in terms of simplicity vs accuracy? Content of this chapter.\n",
    "\n",
    "Fundamental trade-off between **overfitting**, poor prediction by learning too much, and **underfitting**, poor prediction by learning too little. Need to navigate space in-between.\n",
    "\n",
    "Methods to tackle: \n",
    "\n",
    "* **regularizing prior**, same as penalized likelihood in frequentist term, learn less from data\n",
    "* **infromation criteria** to score models\n",
    "\n",
    "**rethinking: stargazing.** common method of model selection: pick model with highest number of significant coefficients. but: p-values not designed to make under-/overfitting trade-off. helpful coeff. can be non-significant, significant coeff. can be unhelpful.\n",
    "\n",
    "**rethinking: is aic bayesian?** not originally derived and often not seen as such, but can be interpreted as special limit of bayes. criterion like WAIC. example of common phenomenon in stats: same procedure can be dried and justified from multiple, sometimes philosophically incompatible perspectives.\n",
    "\n",
    "## 6.1. the problem with parameters\n",
    "\n",
    "Previous chapter: adding parameters can be helpful by revealing hidding effects. also, adding parameters can hurt if parameters are highly correlated. how about case of adding non-highly-correlated parameters? also not always safe.\n",
    "\n",
    "two main problems:\n",
    "\n",
    "* adding parameters nearly always makes model fit better, i.e. reduces train error. McElreath mentions $R^2$ or 'variance explained' metric. Increased for more predictor variables, even if they are random numbers. **understand 'variance explained', intuitive meaning unclear**\n",
    "* more complex models often have better fit, but worse predictions on unseen data = overfit! but simple models tend to underfit. no simple answer, need to make trade-off.\n",
    "\n",
    "### 6.1.1. more parameters always improve fit\n",
    "\n",
    "Overfitting = learning too much from data\n",
    "\n",
    "* regular features: useful, generalize well or inform about quesiton of interest\n",
    "* irrregular features: misguiding, aspects of data that do not generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afarensis</th>\n",
       "      <td>438</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>africanus</th>\n",
       "      <td>452</td>\n",
       "      <td>35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>habilis</th>\n",
       "      <td>612</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boisei</th>\n",
       "      <td>521</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rudolfensis</th>\n",
       "      <td>752</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ergaster</th>\n",
       "      <td>871</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sapiens</th>\n",
       "      <td>1350</td>\n",
       "      <td>53.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             brain  mass\n",
       "species                 \n",
       "afarensis      438  37.0\n",
       "africanus      452  35.5\n",
       "habilis        612  34.5\n",
       "boisei         521  41.5\n",
       "rudolfensis    752  55.5\n",
       "ergaster       871  61.0\n",
       "sapiens       1350  53.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 6.1\n",
    "\n",
    "data = {'species' : ['afarensis', 'africanus', 'habilis', 'boisei', 'rudolfensis', 'ergaster', 'sapiens'],\n",
    "'brain' : [438, 452, 612, 521, 752, 871, 1350],\n",
    "'mass' : [37., 35.5, 34.5, 41.5, 55.5, 61.0, 53.5]}\n",
    "d = pd.DataFrame(data).set_index('species')\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: assuming that brain size is generally correlated w/ body mass, which species have unexpectedly large brains for their body mass?\n",
    "\n",
    "Common strategy: 'statistical control'. Fit lin. reg. between those two vars. Residuals of predicting brain vol from body mass = unexplained variance = come from other factors = unexpectedly high/low.\n",
    "\n",
    "But: why lin. reg.? Why line? Might be some curve relating body mass to brain vol. Why not use residuals of that regression? That could be how things are in nature.\n",
    "\n",
    "Fit increasingly complex models, observe fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.2\n",
    "\n",
    "m_6_1 = smf.ols('brain ~ mass', data=d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspect $R^2$ of model. variance 'explained' by model = lin model predicts some variation of training data. remaining variation = variation of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.490158047949084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 6.3\n",
    "\n",
    "1 - m_6_1.resid.var() / d.brain.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.490158047949084"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my own comparison: \n",
    "m_6_1.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/rethinking_statistics/lib/python3.7/site-packages/statsmodels/stats/stattools.py:72: ValueWarning: omni_normtest is not valid with less than 8 observations; 7 samples were given.\n",
      "  \"samples were given.\" % int(n), ValueWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>brain</td>      <th>  R-squared:         </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 22 Feb 2019</td> <th>  Prob (F-statistic):</th>  <td>0.0798</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:41:32</td>     <th>  Log-Likelihood:    </th> <td> -47.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     7</td>      <th>  AIC:               </th> <td>   98.92</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     5</td>      <th>  BIC:               </th> <td>   98.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td> -227.6287</td> <td>  439.794</td> <td>   -0.518</td> <td> 0.627</td> <td>-1358.154</td> <td>  902.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mass</th>      <td>   20.6889</td> <td>    9.436</td> <td>    2.192</td> <td> 0.080</td> <td>   -3.568</td> <td>   44.946</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   1.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   2.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.399</td> <th>  Prob(JB):          </th> <td>   0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.548</td> <th>  Cond. No.          </th> <td>    215.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  brain   R-squared:                       0.490\n",
       "Model:                            OLS   Adj. R-squared:                  0.388\n",
       "Method:                 Least Squares   F-statistic:                     4.807\n",
       "Date:                Fri, 22 Feb 2019   Prob (F-statistic):             0.0798\n",
       "Time:                        11:41:32   Log-Likelihood:                -47.462\n",
       "No. Observations:                   7   AIC:                             98.92\n",
       "Df Residuals:                       5   BIC:                             98.82\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept   -227.6287    439.794     -0.518      0.627   -1358.154     902.897\n",
       "mass          20.6889      9.436      2.192      0.080      -3.568      44.946\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   1.561\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                2.372\n",
       "Skew:                           1.399   Prob(JB):                        0.305\n",
       "Kurtosis:                       3.548   Cond. No.                         215.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_6_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.4\n",
    "\n",
    "m_6_2 = smf.ols('brain ~ mass + I(mass**2)', data=d).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.5\n",
    "\n",
    "m_6_3 = smf.ols('brain ~ mass + I(mass**2) + I(mass**3)', data=d).fit()\n",
    "m_6_4 = smf.ols('brain ~ mass + I(mass**2) + I(mass**3) + I(mass**4)', data=d).fit()\n",
    "m_6_5 = smf.ols('brain ~ mass + I(mass**2) + I(mass**3) + I(mass**4) + I(mass**5)', data=d).fit()\n",
    "m_6_6 = smf.ols('brain ~ mass + I(mass**2) + I(mass**3) + I(mass**4) + I(mass**5) + I(mass**6)', data=d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "higher degree of polynomial = better fit, until 0 residuals for 6th degree, $R^2 = 1$\n",
    "\n",
    "however: absurd model. curve follows all data points, but pays no price for absurd predictions like negative brain mass in areas without dat\n",
    "\n",
    "reason: n params = n data points. use unique parameter to remember each datum.\n",
    "\n",
    "**rethinking: model fitting as compression.** model fitting can be seen as data compression. parameters = summarize relationship among data in lossy way. use params to generate new data = decompress. if n params == n data points then no compression, just encoding in different way. consequence: learn nothing. need to use simpler model, but not too simpel. known as minimum description length.\n",
    "\n",
    "### 6.1.2. too few parameters hurs, too\n",
    "\n",
    "overfitting: low train error, high test error\n",
    "\n",
    "underfitting: high train and test error. learned too little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.6\n",
    "\n",
    "m_6_7 = smf.ols('brain ~ 1', data=d).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimate mean brain volume without considering brain mass = regression is horizontal line with wide confidence region. bad for seen and unseen data\n",
    "\n",
    "underfit model = under-sensitive to data, changes too little based on seen data (inflexible)\n",
    "overfit model = over-sensitive to data, changes too much based on seen data (volatile)\n",
    "\n",
    "**overthinking: dropping rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.7\n",
    "\n",
    "d_new = d.drop(d.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3XmMJOl55/fvmxGR931U1n31PTMc6miRWu2sTa6s0wsLoLQrzkIWpZ3FQOvVrGHAsCSPAGqlHVuGtV5JlLUytSMvactDHVhDBCxgl/BSkAaWSM5QIjmc4cx015FZVVlVed+Zcb3+I7K6q7uru6urqruu9wMUqioyMjJypjqeyHjf+D1CSomiKIpy/viOewcURVGU46EKgKIoyjmlCoCiKMo5pQqAoijKOaUKgKIoyjmlCoCiKMo59dACIIT4PSHEthDirT0e+2+FEFIIkR39LoQQvymEuCGE+LoQ4jt2rfsJIcT7o69PHO3bUBRFUR7Vfj4B/FvgB+9eKISYAb4PKOxa/EPApdHXi8C/Hq2bBj4JfBj4EPBJIUTqMDuuKIqiHM5DC4CU8s+B2h4P/SvgvwN230n2I8BnpeevgKQQYgL4AeALUsqalLIOfIE9ioqiKIry5OgHeZIQ4r8A1qWUXxNC7H5oCiju+n1ttOx+yx8om83K+fn5g+yioijKufXmm29WpJS5h633yAVACBEGXga+f6+H91gmH7B8r+2/iHf5iNnZWd54441H3UVFUZRzTQixup/1DjIL6AKwAHxNCLECTANfFUKM453Zz+xadxrYeMDye0gpPy2lvC6lvJ7LPbSAKYqiKAf0yAVASvkNKeWYlHJeSjmPd3D/DinlJvB54CdHs4G+G2hKKUvAvwe+XwiRGg3+fv9omaIoinJM9jMN9DXgL4ErQog1IcQLD1j9T4El4Abwu8B/BSClrAG/Anxl9PXLo2WKoijKMREnOQ76+vXrUo0BKIqyX5Zlsba2xmAwOO5deSKCwSDT09MYhnHHciHEm1LK6w97/oFmASmKopxEa2trxGIx5ufnuWuG4pkjpaRarbK2tsbCwsKBtqGiIBTlnHntNXjmGdA07/trrx33Hh2dwWBAJpM58wd/ACEEmUzmUJ921CcARTlHXnsNXn4ZXn0VnnsOXn8dXhiN6j3//PHu21E5Dwf/HYd9r+oTgKKcI6+84h38P/pRMAzv+6uvesuV80cVAEU5R955xzvz3+2557zlyuGtrKzwzDPPHPj5v/M7v8NnP/vZI9yjB1MFQFHOkWvXvMs+u73+urf8PDqO8RDHce772M/8zM/wkz/5k49/J0ZUAVCUc+Tll71r/l/8IliW9/2FF7zl583OeMinPgWDgff95ZcPXwRs2+YTn/gEzz77LD/2Yz9Gr9djfn6eX/7lX+a5557jj/7oj/jd3/1dvuu7vosPfvCD/OiP/ii9Xg+AX/qlX+LXfu3XAPjIRz7Cz/3cz/GhD32Iy5cv8xd/8ReHfcv3UAVAUc6R55/3rve/9BIEg973V145OwPAj+JxjYe8++67vPjii3z9618nHo/z27/924A3Z//111/n4x//OB/72Mf4yle+wte+9jWuXbvGq6++uue2bNvmy1/+Mr/+67/OP//n//xwO7YHNQtIUc6Z558/nwf8uz2u8ZCZmRn+9t/+2wD8xE/8BL/5m78JwI//+I/fWuett97iF3/xF2k0GnQ6HX7gB35gz2197GMfA+A7v/M7WVlZOdyO7UF9AlAU5Vx6XOMhd0/N3Pk9EoncWvZTP/VT/NZv/Rbf+MY3+OQnP3nfufyBQAAATdOwbftwO7YHVQAURTmXHtd4SKFQ4C//8i8BeO2113ju7o8ZQLvdZmJiAsuy+P3f//3DveAhqAKgKMq59LjGQ65du8ZnPvMZnn32WWq1Gv/kn/yTe9b5lV/5FT784Q/zfd/3fVy9evVwL3gIKgxOUZQz45133uHaOZvTutd73m8YnPoEoCiKck6pAqAoinJOqQKgKMqZcpIvax+1w75XVQAURTkzgsEg1Wr1XBSBnX4AwWDwwNtQN4IpinJmTE9Ps7a2RrlcPu5deSJ2OoIdlCoAiqKcGYZhHLg71nmkLgEpiqKcU6oAKIqinFOqACiKopxTqgAoiqKcU6oAKIqinFOqACiKopwwtuUw7FmP/XXUNFBFUZQTwnVcmuU+zXIf3a8xfSX1WF9PFQBFUZRjJqWkXRvQ2Orh2JJIwk9qPPLwJx6SKgCKoijHqNcyqZW6WEOHYFhnbC5CMGI8kddWBUBRFOUYDPs2tY0ug66F4fcxNhcjkgg80X1QBUBRFOUJskyHxmaPTmOIpgkykxFi6SDCJx7+5CP20FlAQojfE0JsCyHe2rXsfxZCfEsI8XUhxP8thEjueuwXhBA3hBDvCiF+YNfyHxwtuyGE+PmjfyuKoignl+O41Da6rL9bp9sckhwLMX01RTwbOpaDP+xvGui/BX7wrmVfAJ6RUj4LvAf8AoAQ4ing48DTo+f8thBCE0JowP8K/BDwFPD8aF1FUZQzTbqSZrnP2rfqNCt9IskA01dSpMYj+LTjnYn/0EtAUso/F0LM37XsP+z69a+AHxv9/CPA56SUQ2BZCHED+NDosRtSyiUAIcTnRuu+fai9VxRFOcG6jSG1zS626RKKGqQnIvhDJ+fK+1HsyT8C/mD08xReQdixNloGULxr+Yf32pgQ4kXgRYDZ2dkj2D1FUZQna9C1qJW6DHs2/qBGfj5OOO4/7t26x6EKgBDiZcAGfn9n0R6rSfa+1LRnyx4p5aeBTwNcv3797Lf1URTlzDAHNvXNHr2Wia77yE5HiaYCCHE81/gf5sAFQAjxCeDvAd8rb/dfWwNmdq02DWyMfr7fckVRlFPNsVwa2z3a1QHCJ0jlw8RzIXzHNLi7XwcqAEKIHwR+DvhPpZS9XQ99Hvi/hBD/CzAJXAK+jPfJ4JIQYgFYxxso/oeH2XFFUZTj5rqS1ii6QbqSWCZIciyMZpyOmLWHFgAhxGvAR4CsEGIN+CTerJ8A8IXRR5u/klL+jJTym0KIP8Qb3LWBfyqldEbb+Vng3wMa8HtSym8+hvejKIpyi2PbDHtdwvHEkW5XSkmnPqSx2cO2XcJxP6nxMP7g0Q3w2q6N7nu8A8bi9tWbk+f69evyjTfeOO7dUBTlFOrUqlSKq0gpmfvAt+HTtCPZbr/tRTeYA4dASCc9ebTRDS2zxVJjCZ/w8Wzu2QNtQwjxppTy+sPWOznzkRRFUY6AZQ6pFFboNRsEQhFy8wtHcvA3+za1Upd+x0L3+xibjRFJHl10Q8/qsdxaptwrY/gM5hPzR7bt+1EFQFGUM0FKSXN7i9qGN+M8Mz1HYix/6Bk4tuVQ3+zRqQ/xaYL0RIR45uiiG0zHZKW1QqlTQgjBXHyOmdjMY7/8A6oAKIpyBgx7PcqrS6Pr/Umys3MYgeChtrk7mx8gkQ2RyIfQjujuXdu1KbaLFNtFpJRMRCeYj8/j157c/QKqACiKcmq5rkN9Y53G1iaarjO2cIFYOnuobUp3Vza/I4kmAyTHwxj+oxlDcKXLRmeD1dYqlmuRC+dYiC8QNsJHsv1HoQqAoiinUq/VpLy6jG0OiWVyZKZn0fTDHdK6zSH1UhfLdAlGDPITYQLhoxnglVKy3dtmubXMwB6QDCRZTC4S98fvXM91sTc3kY6Df2bmPls7GqoAKIpyqtiWRbW4SqdexQiEmLx8jVAs/vAnPsCga1EvdRn0bIzA0Uc31Ad1bjZu0rE6RIwIH8h+gEwoc8c60rKw1tcx19aRpomWTKoCoCiKsqNVKVNdKyBdl9TEFKnxSYTv4NfkLdOhXurSbZpouiAzNcrmP6Loho7Z4WbzJvVBnYAW4Gr6KvnwnQPT7mCAVSxibWwgHRc9ncKYewo99Xj7AYMqAIqinALWYEC5sEy/3SIYiZGbm8cfOvg1c8e+Hd0AkBwLkciFjiyeuW/3WW4us93bRvfpXEheYCo6hU/c3r7T6WCurmJvbwNg5PMYs7Nofh+D7Zt0mwUS8x88kv25H1UAFEU5saTr0tgqUS9tIHw+crMLxLK5A5+hS1fSqvZpbPeRjiSaDpDMh9GNoxngtRyL1dYqG10v6mw2PstMbAbDd3scwa7VsAoF7FodoWv4p6cxZmbwyT6tjbepbBZp9i1EYopvmz+S3bovVQAURTmR+p02ldVlzEGfaCpDZnoW3X+w6/JSSroNk/pmF9tyCcVG2fxHFN3guA5rnTUKrQKudBmPjDOfmCegBW69vr29jbm6itvpIvx+AhcWMSYmkIMataUvUS1v0bMFdnya9MIlpnLqEpCiKOeMY9vUNtZolbfQjQDjFy4TSR78YDjoWNRKHYZ9B39QY3wmTih6NAO8UkpK3RIrrRVMxyQbyrKQWCBiRLzHbRtrcxOrUMAdDPGFwwSvXkEfy2E11il98z9SqzcYigAidZn87AXGk1G0J5QiqgqAoignRqdeo1JcxbEsEmPjpCenDxzjYA5s6qUevbaJbvjIzUSJJI8um7/cK7PcXKZn90gEEjydeZpEwAudc00Ta20Na30dadloyQShS5fQkjF620tsvPkVGp0elh4jMPltzE/Nk4kd7sa1g1AFQFGUY2ebJpXCCt1mHX8ozPiFSwQj0QNty7Fc6ls9OrVRNv94mHj26LL5m8MmNxs3aZktwnqYZ7LPkA15N5+5vR5msYhVKoEr0XNZ/LOz+AKC+vr71N5bodM3sUM5YovfxsXJKSKB4zsMqwKgKMqxkVLSKm9RW19DIslMzXr5PQeY2uk6Lq3KgMZ2DyReNn8+jKYfzcyertVlqbFEdVDFr/m5krrCeGQcIQROs4lZKGCXK+ATGBMT+GdmkG6PSuEb1LbXGTjgxqbILl5hIpfBf0T7dRiqACiKcix25/eEYglyc/MHyu/Zyeavb3ZxbEkk4Sc1HsEIHM3MnoE9YKW1wlZ3C82nsZhYvDWl06lUMItFnEYTYej45+cwJicxu9tsvPs69XoVCz96+gITs5cZS8ZOVJcwVQAURXmiXNehXtqgsVlC07RD5ff0Wib1Uhdz6BAI64zNHV02v+VaFFoF1jvrAExFp5iLz6ELDXtzk0GhiNvr4QsGCFy+hJHL0KkU2fibL9DqdHD0MKHJZ5mZuUAy8uSv7++HKgCKojwxvVaTSmEFazgY5ffMoOmPfsAe9m3qo2x+w+9jbC5GJHE02fyO63hhbe1VbNcmH86zkFggIDWs4jrdnaiGWJTg00+hJaPU1t+n9pUv0xsMcUNpkhe+m8mpGYL+k32IPdl7pyjKmeDYFpVigU6tghEIMnHp6oHaNNrmKJu/MUTTBJnJUXTDEVxWkVKy1dtiubnM0BmSDqZZTCwSdnWs5SLdUglpO15Uw+w1ZEBQXv0W9beLWI6LiI2TvXCV8bEx9CO6o/hxUwVAUZTHql2tUC2u4jgOyfFJ0hNTjzzI6zguze0+rcoomz8XIjF2dNn8lX6F5eYyXatLzB/javoqcdvAulGgu7UFjKIaZmYYWm3WC39Ds7qFg4Y/O8/UzGWyqcSRTTF9UlQBUBTlsdid3xOIRJmcW3jk/B4vumFAc/t2Nn9qPIx+RNn8zWGT5eYyjWGDkB7i6czTpIY61rdW6e2OapiapFkvsfb2X9BpN5F6gMjU00zMXiIWDh3JvhwHVQAURTlSd+T3CEF2Zp54buyRz453Z/OHogapiQiB0NEcsnpWj+XmMuV+Gb/Pz8XkRXJdDeebK/TbHS+qYXEBbSxLZWuF6pv/AXM4QATjJC9cZ2J6gYBx+g+fp/8dKIpyYgw6HcqFZcx+j0gyTXZm7pHzewZdi1qpy7Bn4z/ibP6hM2SlucJmdxOf8DEXmWG8o+G+VcTsD25FNTjxEFvF92l86cs4joMWz5O/+CHy+ckTNY3zsFQBUBTl0FzHobZepFneQjP8B8rvsYYO9U0vm1/XfWSno0RTRxPdYLs2hXaBtfaa13/Xn2OypcHyBvZOVMPFi/Q0h8Lqu3Te2cCVglB2lqm5q6RT6UPvw0mkCoCiKIfSbdQpF1ZwLJNELk96auaR8nsc26WxNcrmF5DKh4nnjia6wZUu6511Cq0ClmsxJuJMNjW0yjZyFNVgTE/T6DcoLr1Jv11HaH5iU9cYn71EOBw59D6cZKoAKIpyILZpUimu0m3UvPyexUsEo/vP73FdSavcp1nuI11JNB0klQ+jGYef2XNP/11TZ6YZwt9o3opq8I3nKVdL1N/6c+xhDy0QIXvhO8hPL2IYR3Mz2UmnCoCiKI/Ey+/ZprZeREpJemqG5Nj4vqd27kQ3NDZ72LZLOOYnNRE+smz+2qDGUmOJjtkm2ra52jKI9C2EITHm53DSKTZLSzS/+nWkYxOMZchf/Hay+elDtZc8jVQBUBRl38x+j+3VZYbdjpffMzuPEdx/zEG/bVIrdTEHDoGQTm42RjB6NGfbbbPNUnOJeq9KqN7nYkMj7vrRgn6MSzN0g3421t+nt/xlAKKZKXJz10ikDhZDcRaoAqAoykNJ16VWWr+d3zN/gVhm/wdOs29T2+zSb1vofh+5mRiRpP9IBnhv9d9tlQiUm8w1BRktjhaNoc/M0pAm1eK7mO0yPk0nPXWJsdnLBMOxQ7/2aacKgKIoD3SY/B7bcmhs9ejUhghNkJ4IE8+EjiS6wXIsVlorlGqraKUKU20fOX+KQC4HE5OUe3UaN7+CO+hgBELkLzzL2PQlNONoppSeBQ8tAEKI3wP+HrAtpXxmtCwN/AEwD6wA/0BKWRdeOf8N4IeBHvBTUsqvjp7zCeAXR5v9F1LKzxztW1EU5Sg5tkV1rUi7Wn7k/B7XcWmOBngBYtlRNv8RRDfYrs1ae421zfcQG1uMdQRj4Tyh2UmsXI71aonO238Bjkk4liR74btJ5WcRvqO5e/gs2c8ngH8L/Bbw2V3Lfh74f6WUvyqE+PnR7z8H/BBwafT1YeBfAx8eFYxPAtcBCbwphPi8lLJ+VG9EUZSj065VqBZu5/ekJibx7eMAKqWkXfWasji2F92QHA9jHEF0gytdSt0ShcJbuMV1kn0fE7FJIlfmacdiLG+tMPzGW/hwSWQmyc1dIZoaP/TrnmUPLQBSyj8XQszftfhHgI+Mfv4M8Gd4BeBHgM9KKSXwV0KIpBBiYrTuF6SUNQAhxBeAHwReO/Q7UBTlyFjDAeXVFfrtJoFIlInZBQLh/eX3dJtD6ps9rKFDMGKQnw8TCB/NAO92d5uVm1/FKhaJmoLx5CzxS5eoGj7WN27iFrbQdY2xyXlys1fxRx49afQ8OugYQF5KWQKQUpaEEGOj5VNAcdd6a6Nl91t+DyHEi8CLALOzswfcPUVRHoV0XRrbm9RL6wgeLb9n2LOolXoMuhZGQDvSbP56t8ry+19msLpK0PExnZkjPv8U23afjY13EIMmwUCAzIVnyExdxOc/vcFsx+GoB4H3+muRD1h+70IpPw18GuD69et7rqMoytEZdDuUV0f5PYkU2dn5feX3WKZDYyebXz/abP52p8byu1+ms3oTwxXMjF8iPH2Fcq/B5vIbaPaARCxG9sJ3Ec8vgLq+fyAHLQBbQoiJ0dn/BLA9Wr4GzOxabxrYGC3/yF3L/+yAr60oyhFwHYfaxhrN7S00w9h3fo/j7IpuAJJjIRK5EL4jGODtteqsfutL1Avvo0kfEzOXMfIXaTQrVG5+CU3YZNN5crMfIpSahFOWv3/SHLQAfB74BPCro+9/smv5zwohPoc3CNwcFYl/D/wPQoidv67vB37h4LutKMphdBt1KoVVbGtIPJcnPTmNpj/4cOBl8/dpbPdxHUksHSCZD6Mbhz/7HtSqFN/9CtW190H4SE9fwcgu0KiW4OaXCOg+Jiamyc5eRY+ezWC247CfaaCv4Z29Z4UQa3izeX4V+EMhxAtAAfj7o9X/FG8K6A28aaA/DSClrAkhfgX4ymi9X94ZEFYU5cm5I78nGGJy8SlC0QffECWlpNswqW91sU2XUMwgPR7Bf8hsfiklZnmbjXe/yvbmEo4Gkbkr6LFJutV1tJU3iAb9pBcvk566jAjsP2dI2R/hTdg5ma5fvy7feOON494NRTn1pJS0K2Wq60Wk65KamCSZn3ho9s2gM8rm79v4gxrpiQih2OFupJKui1Uqsfn+19iqrGAZAjE+jxHKQm0Tw+mQiEbJzlwimr8A2vkIZjtKQog3pZTXH7aeuhNYUc44s9+jvLrCoNsmFIuTm114aH6PObCpl3r02keXzS8tC2tjg8rS22zWi/QMyXB6GsMXI9qp4+/eJJVOk515lkBqBs5ZMNtxUAVAUc4o6brUNzdobJYQPh+5uUXi2dwDn+NYLvWtHp3aAOETR5LN7w4GWMUijcINNlvrNIKCVj5DiBCZbpuQNiA9MUlm+jJabOzhG1SOjCoAinIG9dstyqsrWMM+0XSW7MzsA/N77s7mj2VG0Q36wc/CnU4Hq1Cgs1Fgs7vFVtClmo0SczWmeibJMKTnL5CcugzB+IFfRzk4VQAU5QxxbJvqWoF2tYzuDzw0v+eebP64n/REBCNw8Jk9dr2OVSjQL2+x1S+zErSpJAUpW+Py0CETD5G5+BThsQtg7D9KWjl6qgAoyhnRrlWoFgs4tk0yP0lq8sH5Pb2WSb3UxRw6BMI6ubkYwcjBBlyllNjbZazCKmarwZbZ4D1jSDkyJGvZPOtEyafTZGauYKRmQVOHnpNA/V9QlFPOGg6oFFbptRoEwhEmLl19YH6P2beplbr0OxaG38fYbIxI8mDRDdJxsEolrGIRp99j0+nwTa3DttEmZ1t8VyjN1PgUyanL+OIT6satE0YVAEU5paSUNLc2qZXWAMjOzBHP5e87U8c2Heqj6AafJkhPRIhnDhbd4Jom1to61voarmVRwuTr1Ki5FTKWxXPRPHMLl4hOXIKwunHrpFIFQFFOoWGvS3llmWG/SziRJDs7j+Hf+yzecVya231aFS+bP5ELkRgLHSib3+31MItFrFIJ6bis6/B1uUnT2iLpmvyd5DQXZj9AYOwC+COHeo/K46cKgKKcInfk9+g6+cVLRFN7n2FLV9KuDWhs9XAcL5s/NR5GP0A2v9NsYhaK2OUyDrBm6LzlbtDprRPH4bnMLJfnr6On50FXHbdOC1UAFOWU6DUblFdXvPye7BjpqZn75vd0m0PqpS6W6RKKGqQmIgQeMbpBSolTrWIWijiNBkMEa0aAd81VOq0VEgZ8T/4CVxY+hJaYUYmcp5AqAIpywtmWRbW4SqdexQiEmLxy//yeQdeiXuoy6Nn4Axr5+Tjh+KOdkUvXxd7cxCwUcXs9OtLHeiDE8nCJTmuJuF/je2avcGnub6HHxtXA7mMgpQRHIg5xH8Z+qAKgKCeU116xTHVtJ79nmtT43vk91tChvtml2zTRdHGg6IadqAZzbQ1nMKQh/JSMMFvmTVqtZWJBjQ8tfoBLs9+DP/LgO4qVRyddiduzcNoWbttEBDQCc4/3BjlVABTlBDIHfS+/p9MiGI2Tm5vHH7y325Vj78rmF3jRDdngI2Xzu8MhVrHoHfyHFhUtRNkI0rKWaXULhEM63z79QRZnv4dwSM3oOUrSlbgdC6dt4nZMpCPBJ9CiBtojfnI7CFUAFOUE8fJ7SjQ2N0b5PQvEMrl7zuSlK2lW+jS3veiGaDpIMh96pGz+nagGa2uLnuVQNqI0Az765k06w00CYZ0rc8+yOPW3iKsD/5GRtovT8c7yna4Frnepxxfzo8X8+CLGkXRV2w9VABTlhOh32pRXlr38nlSGzMwcunHnnbleNr/XfN22XMIxP6mJMP7g/v8p70Q1WNUqraHDdiDKIGjiWO/Ro4ov7mcq+wEW899JRl3qORLScnHapnem37NAgjB86MkAvpgfX1g/VNLqQakCoCjHzLFtautFWpVtdCPAxMUrhBPJe9brd7zohmHfIRDSyM7ECUX3d5ngjqiGZouaCeVgGBnpo9s3sahjpwLEk0+xkP8g+fD9byhT9scdOt5ZftvE7dsAiICGngl5Z/qHbKhzFI5/DxTlHOvUqlSKq6P8nglSk1P35PeYg1F0Q9tCN3zkZmJEkv59HaB3RzX0210qto9qIIAR7RKRS/T0Du1EACNxlcX0VaZiU/jE2czhf+01eOUVeOcduHYNXn4Znn/+aF/DHdg4Le+gL4cOAL6Qjj4W9g76hwjZexxUAVCUY2CZQyqFFXrNBoFQhIlLVwiE77xz1rYcGls9OrUhQhOkJ8LEM6F9XR+Wpom5to61vk6706Ms/TQDfsKBJnm9TD84oBb0Q/Qic4kFZuIzGL6z23nrtde8A/6rr8Jzz8Hrr8MLL3iPHaYISClxezZux8RpmUjLBQG+sIGeCqLFDMQR9Ex+XFRLSEV5gqSUNLe3qG0UAUhPzpAYu/Nyi+u4NEfZ/ICXzT+2v2x+L6phDXNjg0Z3wJYvhBV0iehN0gGbYcih5Ndxw2nGo5PMJ+YJaAcLgjtNnnkGPvUp+OhHby/74hfhpZfgrbcebVvSlbjdnZk7FtJ2vZk7EcMbyI0aj33+/sOolpCKcsIMez3Kq0sMe13C8STZ2TmMwO08fCl3RTfYkkjCT2p8f9n8O1ENg60tKl2LshGCqEHKVyUX8dGP6BR1P6Y/RDacYyGxQMQ4P1k977zjnfnv9txz3vL9kI70zvJ3Zu44EjSBFt01c0c7fWMmqgAoymPmug71jXUaW5tefs/CRaLpzB3r9FomtVIXa+gQDOuMzUUems2/O6qhU65Q6bvUggGMhENeL5OLBTDjKZZ8Lj2fRiKQ4OnEIonA/RvEnFXXrnmXfXZ/Anj9dW/5/XjTNb1LO27PvjVdU4sH0GIGvvCTm675uKgCoCiPUa/VpLy6jG0OiWVyZKZn78jvGfYsaqUeg+4om38uRiTx4Esy0nWxt7YYFgo0Kk3KJnSCOuGUxZy/Si4eZpiY44awaLkmYT3GM8lFsqHs4367J9bLL3vX/O8eA3jllTvXk5ZzaxDX7dvedE2/Dz0VQIv5EaHjma75uKgCoCiPwT35PZevEYrdvq3fMh0ao2x+TRNkJiPE0g/O5t+JahgU16jWwuZaAAAgAElEQVTW2pRtH1ZUJxZrcznkkk7EMROzvI9F1Wzi9/m5krrCeGT8TB20DmJnoPell27PAnrlFW+5O7S9+IWWiTvwpmv6ghp6djRd8xHusThtzu47U5Rj0qqUqa4VRvk9U6TGJ2/l9ziOS3OrT6vqDfAmx0IkcqEHRjfsRDV0C2tUmj3KwkBENNKhLvmwIJFMYyWnWHKHbPa38QkfC4kFpqPTaCqh85bnn/e+pJTIvo3TsRjcMJHmaLpmWMfIh70bsw4QmX0aqQKgKEfEGgzYXl328nsiMS+/J+S1ZpSupFUd0Nju4TqSaGqUzf+AKYI7UQ3NwgblzpC64ceIa0yFO2QjOtHUFFZyhhWnx1rH6wo2FZ1iLj6HoZ3dKZ0HsRO0tpO7c2u6ZsRATwe9yzvG2bz/4UFUAVCUQ5KuS2OrRL20d35PtzGkttnFHmXzpyci+B9wF6hdr2OurlJd36LctegG/YQzPi6E22RjIfypC7jJWYpWm9XWDWzXJh/OM5+YJ6TfGxh3Xt2artm6N2jNNzaarnmArmhniSoAinII/U6byuoy5mCU3zM9i+734hkGXYvaRpdh38Yf1BhfiBOK7R3dsBPV0F9dpbxRoTJ0saMG8XG4Gu6RjoXR0k8jE7NsWk2W6+8wdIakg2kWE4tE/dEn+bZPLOmMgtZau4LWNIEv6keLP9mgtdNAFQBFOQDHtqltrNEqb6EbAcYvXCaSTAFedEN9s0evZaLrvgdm83tRDZu0l5bZrjSpWSBiGpmcy3hkQDwWR6TnIT5NxWywXHuLrtUl5o9xNX2VVDD1hN/5ySOt0XTNtonb3SNoLaSrg/59qAKgKI+oU695+T2WRWJsnPTkND5Nw7FcGtteNr/wCS+bPxfCt8fBZyeqoXpzhUq9QxMNf9LHVHzIWMQlHM9CegGieZpmi+XqWzSGDUJ6iKcyTzEWHjuGd35yuOauoLXeKGjNPwpaixpnbrrm43KoAiCE+G+AfwxI4BvATwMTwOeANPBV4L+UUppCiADwWeA7gSrw41LKlcO8vqI8SZY5pFpYpduse/k9Fy8TCEdwXUljq0ez7GXz34pu2GNQ0e31GBaKbC0VqDR79P1+whmNi3GTbMTASMxAagHCaXpWj+Xq25T7Zfw+P5dSl5iITJzZsLaHcQe2d8Bvm7iD0cyd4E7QmoEvoM5nH9WB/4sJIaaAfwY8JaXsCyH+EPg48MPAv5JSfk4I8TvAC8C/Hn2vSykvCiE+DvxPwI8f+h0oymMmpaRV3qK2voZEkpmaJZEfB/CiGzZ72LZLOO4nNb53Nr/TatFdWmZrZYNK18KN6CQmDRZiJqlIEF/yEqTmwR9h6AxZqb3LZncTn/AxH59nOjaN7jtfB7hb0zVHZ/rSdAFv5o6RD5yr6ZqPy2H/onQgJISwgDBQAv4u8A9Hj38G+CW8AvAjo58B/hj4LSGEkCc5jU4593bn94RiCXJz8xiBIP22F91gDhwCYZ3cXOye6IadqIb6jWW2ils0TQcRNcjOCSZiDrFIGFJXIDEDuh/btSk2lym2i0gpmYxOMhefw689/taAJ8WtvritO4PWfGHduzEr6j/2oLWz5MAFQEq5LoT4NaAA9IH/ALwJNKSU9mi1NWBq9PMUUBw91xZCNIEMUDnoPijK4+Ll92zQ2CqhaRpjCxeIpbOYfZvNpSb9joXu9zE2GyOSvDO6Qbou1uYm5feWKG/V6NgSI6kzMyPIx1wC4aR3th+fAp8PV7pstNdYba1iuRa5cI6F+AJhI3w8b/4Jk47E7Zo4bQunY94OWosYo5k7/lMZtHYaHOYSUArvrH4BaAB/BPzQHqvunOHv9X/wnrN/IcSLwIsAs7OzB909RTmwvfJ7pBSUi2069SE+TZCeiBDP3BndIC2L/to6W99aolxrY2o+wikfF9MO2Sjo0XHv+n4kC0IgpWS7u8Vya5mBPSAZSLKYXCTujz9g786G+/XF1Y6hL+55dphLQP8ZsCylLAMIIf4d8D1AUgihjz4FTAMbo/XXgBlgTQihAwmgdvdGpZSfBj4NXj+AQ+yfojwSx7aoFAt0ahWMQJCJS1cJRmJ3ZPMnsiES+RDarhuI3OGQ9tIqm+8tU231kAGd1ITGZNolEdIR8RnvjD94O4WzNqix1FiiY3WIGBE+kP0AmVDm7l06U6TleGf5d/fFTQXwRY+vL+55dpgCUAC+WwgRxrsE9L3AG8AXgR/Dmwn0CeBPRut/fvT7X44e/4/q+r9yUrQqZWprBVzXJTk+SSo/Sadhslas4ziSaDJAcjyMsWvQ0el0qb1/k82bRZo9E19EY2zBYCIhiQQD3rX91BwYt+/ObZttlppL1Ad1gnqQa+lrjIXHzuyBzx06t2fu3N0XN362g9ZOg8OMAXxJCPHHeFM9beCv8c7c/x/gc0KIfzFa9uroKa8C/4cQ4gbemf/HD7PjinIUrMGAcmGZfvt2fo9lamzcaGKZLsGIQX4iTCB8e4DXqtXZfOd9tgslepaLP6Exf0ljLK7hD4a9s/3EDGi3/3n17T7LzWW2e9sYPoMLyQtMRc9m/11398ydXX1xjbFR0NoJ64t7nqmWkMq5dEd+jxCkp2bwh1M0NnsMejZGQCM9ESEc92bgSCnpl7YovXODymYFy5VE0hpTeUkmYqCFEt6NW7EJ2HU2bzkWK60VSt0SANOxaWZiZ6v/7q2+uDsH/V19cXeu6Z/HoLXjpFpCKsp9DDodyqtLmIM+kWSaRH6adtWittlC0wWZqVE2vxBIx6G5usbG2zep15qgQXpMYyKLd30/mvcO/OH0Ha/huA5rnTUKrQKudBmPjJ+p/rsP7IubOxl9cZWHUwVAOTdcx6G6XqRV3kIz/OTmLmKZAbaWu8Cd2fzucMjW+ytsvrdEpz3AF5RMzOqMZwThgN+bwpmah8CdIWyudNnsbrLSXMF0TbKhLIuJxTMxpVM6Lm7H4k//2OTf/b7F+ppkYkrwoz/h5z//+358UTVz57RRBUA5F7qNOuXCCo5lEs/m0QMZ6lsm0hkQTQdI5r1sfqvTpfjODbZuFjAHJoGYj4WLGmMpDcMfgOQcJGdBv/dMvtwrs9Rcom/3R/13nz71/Xfv7ov7538m+T8/4+Of/UKA6/+Jwf/3VYMX/rGgG77ddUs5PdQYgHKm2aZJpbBCt1nHCIaIJKfptcC2XEKxUTZ/UKdbrbPx9vuUVzdwHYdYUmNqwkc6puMLRLzLPPEp2KPDVmPQ4GbzJm2zTcSIsJBYONX9d/cOWvPm6P+dH/TzP/5LnY/+3dtn+l/8otdq8a23jmuPlbupMQDlXPPye7aprXuxCpHkOI4dpVV1CYQ0sjNxQlE/tfVNNr75Po3NCgKXTE5jKq8Ti/ghlPYO/JHcHQO7Ozpmh+XmMtVBFb92uvvv7h20pqHn7uyL+5Wvw3N/587nPvec12dXOX1UAVDOHLPfY3t1mWG3gxGIohk5+l0fuiHIzUQIxXS2l9d45+0b9BptNM1haspgIqcTDBgQy3t37IaSe25/YA9Yaa2w2d1E9+ksJhaZik6dqv67dwatWfvui3vtGrz+Onz0o7eXvf66t1w5fVQBUM4M13WolzZobJYAgR7I47pRpCVIjYcIRn2U3l9l+90lrF6fUMhlcd5gLKuj+4OQmPYGdo292yparkWhVWC9sw7ATGyG2fjsqZnSuWdf3J2gtcyoL+5DZu68/DK88AK8+qp35v/6697vr7zyhN6EcqRUAVDOhF6rSaWwwrDXR/hi+PQM0tWJZYIYYcnGu+9TvbmKtCwScZi87COdDiOM4K4bt/Y+kDuuw3pnnUK7gO3a3pTO+DxBPfhk3+QBSFfeOuDf3RdXGxvN3HmEvrg7A70vveRd9rl2zTv4qwHg00kVAOVUc2yL6lqRVqWMbfrQ/ONo/iiRhB9XM1l/722axQ00aZFL+5iY0InF/BCIeZd5YhPg2/sAKKVkq7fFUnMJ0zHJBDMsJBZOfP9d6bg47buC1jSB74iC1p5/Xh3wzwpVAJRTq12tUC2u0msPESJBIJojEDEYOl1W336H4XYF3Wczk9eZGA8QCBregG5qASIPDl6r9CssN5dv9d99Kv0UyeDeYwIngbTcW/ELdwSt7fTFVUFryh5UAVBOHWs4oLy6QqtSwxzoBGMz6KEg/UGTrbdWcFptwrrFhWmdsVwALRiA+OToxq3YA7fdHDZZai7RHDYJ6SGezjxNLpx7Mm/sEbmmg9sycTq7pmvuBK3F/Iigpg76ygOpAqCcGtJ1aWxvUikU6TZM9GAWI56g1anSX1pD9Pskww6TizqpTARhBLybtlJze964tVvP6rHUXKLSr+D3+bmcusx4ZPzEhbW5AxundVfQ2q2+uCpoTXk0qgAop8Kg22FreYn6RgPHDSDC0zQ6NdzVG+jukPGEZGJWI5KKIPyR0cDu9J43bu02dIasNFfu6L87E5s5MVM6bwWtje7G3R20pqeCaDEDYZyMfVVOH1UAlBPNdRyqa0VKN4v0Wg62kcTs9dA2v07IZzGegXxeIxALQijlXd+Pju1549ZulmtRbBdZa68BMBmdZD4+j3GfmUBP0v364npBa4bqi6scGVUAlBOr26hTfPt9qptNekMDx3IJ2cukAhYTkxrZnI4eDnoH/PSCVwAewpWuN6WzVcByLcbCYywkFgjpe8/9f1Ju9cVtjWbu7PTFjfrRYobqi6s8FqoAKCeObZpsvHeTlffWaFR7CEcnqneZiphMzGgkskF8gV03bvkfnrQppWS7t32r/24qmGIxsUjM/+BB4cdpJ2jNbVv39sWN+/GFVbqm8nipAqCcGFJKKmslvvnGO1TWKmgWJKMGM5kB4+M6kUwY4Q95B/3k7H1v3LpbbVDjZuMmXatL1IjybO5Z0sH0w5/4GEjL8c7yO9bt6Zp+ry+uFvMjQmq6pvLkqAKgnAitRpu/fv1v2HxvBW1gk0lHWJyW5MddAskoIhgfddyavO+NW3drm21uNm7SGDaOtf+uO7S9G7NaJu5g13TN7J1Ba4rypKm/POVYtbpD/uYvv0Hpb96GbpexdIzLlyKM5X0YiQiEs6NEzv3HK/esHsutZcq9MobP4GLyIpPRySc2pVNKiRw49/bFDY/64sb3DlpTlCdNFQDlWFTaA9756xuUvvLXyFaTbDzAU9+RYGwyhBYNQXzCm9ETjO97m6Zjev13OyWEEMzGZ59Y/13pStz+Hn1xIzvTNVVfXOXkUQVAeWIcV7LZGnDjG6tU3/waTn2bVEhw9dviTF1I4AtFIDnjdd0y9h+0Zrs2a+01iu3iE+2/e0df3PauoLWIgW9s1Bf3EYLWFOVJUwVAeewGlkOx1mP93QLtb7yHVSkRM0wWr0WYuzaGHkt4A7vxadD2/yfpSpdSt8RqcxXTNcmFciwkFh5r/92dvrhO2xvIvRW0Fh3N3Dlk0JqiPEmqACiPTWtgUdhuU76xyuDGGm5li7DoMD6ns/jsHKHMqPFKbPyhN27dbbu3zXJzmb7dJxlIPtb+u9LeFbTWvStoLWqo6ZrKqaUKgHKkpJSU20MKW03aywWcwhaiWSfk1EjkJLPXxknPX0SkFyH86FMx64M6S82lW/13P5D9AJnQg5M9D2LvvrgaejqopmsqZ4YqAMqRsB2XjcaAYqmGWSigb9UIdru4VgV/zGZ8PsX4tWcx8pfAH3nk7XfMDkvNJWqDGgEtwNX0VfLh/JEehB/YFzfuxxdQ/1yUs0X9RSuH0jcdivUeG2vbiI11grUGoYGJazdw/X2yMxEmnvkg0bkPgO5/9O3bfVaaK2z1ttB9OhcSF5iKTR3JlM47++KaSNMFHt4XV1HOClUAlAOpd00KtR7V4ib61gbJXhu/aeG4A4Z6l1BWZ+zKddKXvxPN/+gHfsuxWG2tstHdAGA2NstM/PBTOm8FrY06Zu0ErfnCundjlgpaU84RVQCUfXNdyVZ7QKHcobuxSbCyyaTVxTCHWAgGeg8tLMjNXmXs6rcTiu1/Dv8Ox3VY66xRaBVuTemci88dqv/uA/vixvz4oipoTTmfVAE4Jq+95jXT3mms/fLLJ7fPqmm7rNV7rFXaOKUSkdoW8/QwbJshOsOAD8dwiObnSM9fJjU+gdhnXMMOKSWb3U2WW8u3+u8uJheJGI8+XgD36Yur+46sL66inAWHKgBCiCTwb4BnAAn8I+Bd4A+AeWAF+AdSyrrwRut+A/hhoAf8lJTyq4d5/dPqtde8A/6rr8Jzz8Hrr8MLL3iPnaQi0B5YFGt9NisNfJubpFplcqKLD4eeCDKIRLF8LnoqQSKTIzc3jz/46LHKlX6FpcYSPbtH3B/n6czBpnSqvriK8miElPLgTxbiM8BfSCn/jRDCD4SB/x6oSSl/VQjx80BKSvlzQogfBl7CKwAfBn5DSvnhB23/+vXr8o033jjw/p1UzzwDn/oUfPSjt5d98Yvw0kvw1lvHt18wSuTsmBTrPerlBv6tDbKDBlnRQfhsuiKG449g+w1kwIc/EiIzPUM8O/bIr7W7/25YD7OYXCQb2n/mD4A73DVds387aE3bOdMPqQ+5yvkjhHhTSnn9oesdtAAIIeLA14BFuWsjQoh3gY9IKUtCiAngz6SUV4QQ/9vo59fuXu9+r3FWC4CmwWAAxq7xTMuCYBAc53j2yXElG40+xVqPQbVGqFJibNgirbURuktHS2JpcWQkgatZCN0hmsqQmZlDNx5tYLZrdVluLnv9dzU/C/EFxiPj+z4737Mvbki/fXlH9cVVzrn9FoDDnB4tAmXgfxdCfBB4E/ivgfzOQX1UBHZODaeA4q7nr42W3bcAnFXXrnmXfXZ/Anj9dW/5kzawHO/6fq2HW6mSqG8x47RIihYiKuhoKfoigy8+hi9gYg2bGIEAudmLhBPJR3qtu/vvLiQWmI5OP7T/7q2+uHcHram+uIpyKIcpADrwHcBLUsovCSF+A/j5B6y/1+ndPR8/hBAvAi8CzM7OHmL3Tq6XX/au+d89BvDKK09uH5o9i0Ktx3aji1Ytk2mVGXPbRH0tfAmdjpajK7OI+Dj+KJjdLVzTITU+QWpyCt8jNE23XItiq8hax+u/OxWdYi4+98D+u3cErd3TF3cUtKamayrKoRymAKwBa1LKL41+/2O8ArAlhJjYdQloe9f6M7uePw1s3L1RKeWngU+DdwnoEPt3Yu0M9L700u1ZQK+88vgHgF1XUu4MKdR6NJs9/NUtpjtVsrJJSOuiZUP09Emadh7COULJANZwi0GrRSAUITe/QCC8/1k5O/13V1ur2K5NPpxnPjF/3/670pG4He8s/96+uKOZO2q6pqIcmQMXACnlphCiKIS4IqV8F/he4O3R1yeAXx19/5PRUz4P/KwQ4nN4g8DNB13/P+uef/7JzfixHJf1ep9ivYfZ6RGpbnGhXyXlNAj4B+jZGAP/PFVrHNtIEc4H0bQOrcpNADLTcyTG9h+7IKVkq7fFcnOZoTMkFUxxIXGBqD9677qjvrhOa5S5o/riKsoTc9gpEi8Bvz+aAbQE/DTgA/5QCPECUAD+/mjdP8WbAXQDbxroTx/ytZWH6A5tivUepcYAt90m1dxmsV8j7tYxgjbGWIJhcJ7tYR5TxAikDOIJaJeLdPpdwvEk2bl5DP/+c/Wr/SpLzaVb/Xevpq+SCqbuWOeOoLW+rfriKsoxOVQBkFL+DbDXSPP37rGuBP7pYV5P2Z9qZ0ix3qfSHqK36oy1K+SGNcJ2DSMqMPJprMgU5eEYfSuIEfSRzQcxuxUqhU00XSe/cJFoev8pmy2zxVJjicawQUgP8VTmKXKh3K0DuTv0Zu64betWX1xfUPXFVZTjpP7VnRGuKym1BhRrPTo9k2CrxnynQtqs4rcb+JMGxlgeJzFHdZCj0xFomiAzGUbTBlSK72FbQ2KZHJnpWTR9f38aPavHcnOZct/rv3spdYmJyAQCgezb2B0Lp2UizV19cVXQmqKcCKoAnHJD22Gt3met3scamsRaVS53KiSHFTTZwp8Jo+dncRPz1K0crZoFQCIXIpLSaWwU6dSrGIEQk1eeIhSN7et17+6/OxefYzoyjW8gcbb69/bFzQS9oDXVF1dRTgxVAE6p9sBitdpjuz3AHQzJdWqMdbeJDipovi7+fAI9fxWZmKdtpWiUBziORTQZIDUepteqsvGtItJ1SU1M7zu/x3Ztiu0ixXYRKSUT4XFmtCm0DjibbexdQWuqL66inGyqAJwiUnrTOIu1HvWuhT7sM9mtku1sEeiX0QM2/ukU2tglSC/QtWPUS10ss08oapCaiOATNlvL7zHotAhGYuTmF/aV3+NKl43OBqutVSzLYowM03Ico6yBO8RVfXEV5dRRBeAUsB2XUtO7vt8zHUKDLovdMun2Jvqgih6R+C/k8OXmIb3AwA5SK3UZ9tr4Axr5+TihqE5jq0S9tIHw+cjNLRDL5B4620ZK6fXfrS1ht4YkrRgT2hxhLYQwfGhJv3c9P6Srg76inDKqAJxgO9221ht9HNslNWgx16kQ65TwDWoYST/++UlEbhGSs1iuQX2zS7fZRNd9ZKejRFMBBt0Oa+98C3PQf6T8nmqrQmFjiWGzR9gJMh6eJx5NosVHl3bUdE1FOdVUATiBGj2v21a5PQTHIW+2GGtuE2pv4LMbGNkYxuULiOwiJGZwXEFjq0e72gEBqXyYeC6EdB0qhRValW10I8D4hctEkqkHvrY7sGnV6qxvFuh22hg+g7n0HNlcXvXFVZQzRv1rPiFcV7Ld9mIaWn0LXTrMDupkmpsYrRIaHYyxJPrks4jMIkTzuBJalT7N7T7SlUTTQVL5MJrho1OvUSmu4lgWibFx0pPT+LR7p13u7ovbb3TYbJRomA1ESGNifpaJ/Ax64HBtGE9T8xtFOU9UAThmpu2y3uizVu8xtFzC0uJSv0ayto6vvYnuNzGmMuiTT0FqHsJppJR0G0Pqmz1syyUc85OaCOMP6ljmkO0bK/SaDQKhCBMXL9+T33OrL27LC1qzTYvtwTZlathxmMjPMJOcPXT/XTg9zW8U5Tw6VEOYx+2s9gMA6AxtirUepWYf14W0MJnoVomWV6G7jRERGBNjaBMXvAO/3zuI9zsm9VKXYd8hENJIT0QJRg2klLTKW9TW15BI0hPTJPK3M/alcztd0+mY4EhcIan46qzLTayQy3hsnPnEPAFt/9EPD3OSm98oyln1JPoBKI9ISkm1613fr3VMfD4Ydwfk29v4y6uIfhkjGcR4agZf/hIkZkD3A2AObGqlLv22hW74yM3EiCT9CCEY9nqUV5cY9kb5PbNzGIEg0naxO+Y9fXG1mEHZV2fVLmJKi2woy2JikbARPvL3/M473pn/bs895y1XlP+/vXuPkWSvCjj+PVX9fs37ufPeu+wuuyAIFyEYI1diFBA0Si5BBQ0Rn0RjFEQToxFiMCpXNMEgqAiYqxJRg0YlAsaAIiAL916W+9h5z867p3v63dVVP/+omtnZ3Z7dnX117/T5JJPpqqnpPvOb3Tldv/rVOaq1NAHcB65nWM1XWMyWKddcojbMUKJvexVrawFxdoj0ZwifPoMMPASZExDclNVwXHLrZYrZGmILvSMJMn1xxBI8zyW7cpnc+iq2bTM4fZJUuge34FBb223aF3ebHeZ2n6HiVOiKdnGua+a2+u/eqnZqfqOUupomgHvI77blz+83XEM6DGcpkFlbhOwylikSGewmNP4SpO8kpAb2v9dzPfKbFfKbFQDS/TG6BxPYQROU8m6ezYU5GvUaqUwf3ekhyHtU13KA3xc31Bf3V+7EQuSqOWbzT7Nb3yURSnC+//yR++/ejnZofqOUak4TwD2QrzgsZcus71YBGIjCiLNDfGEOk1vBDjlERvuxx1+I9M5ALLP/vcYYCtkqufUybsOQ7IrQM5IkHBROcxsOW0uLFNc2CXsRBjLjhJ0oXraOFQ8RHgwKrQV9cUtOidnNi2xXt4nYEU73nD5S/9071armN0qpm9MEcJcY4y/jXMqWyZUdbFsYi8FwYQtr/lkorGEnLCInh7BPnPYv7IZjVz1HKe+v7HFqLrFEiMHJJLFkeP/580tr7MwuQ8Ulne4l1duLnfS7ZdnpqwutVRtV5nf9/rshK8RM1wwnUidu2n/3XrifzW+UUrdOE8AdclyPy7kKS9kKVcclHrE5Fffo21nFPPOcv6KnJ07k3BTWyGn/wq599bDXyg7Z1TLVkkM4YjE4mSbZFcV4BrdQp7ZVZGduiXqpTDgWp+fUFNH+TNO+uI7nsLi7yEpxBYDx9DgTmbuzpFMpdbxoArhN5XqDpWyFy7kKrmfoSYR4KOSQ3pjFW52FWpZIf4bI2Rcig6cgPQzXTLs4dZfcWplirhbU5k+S6opgyg3qywXcQo1iNksxl4W4Tc/5STITw1hNqmu6nusXaytc6b873TVNLBS77lillAJNAEeWDZZxbhVqWBYMJsKMNgpEF2fxtubBLRId6iU8+XJk4CQkeq97Dtf1yK9X2N32L/B29UZJxWxM2aG2VQHP4Dg18oV16laN5Jk++scnCUUi1z3XXv/d2fwsdbdOb6yXma6Zpv13lVLqIE0At8DzDGu7VRazZYrVBuGQxVRPlOFyFp55GrOzjNh1YiMDhKZeivROQ7RJA3TPsLtdJbdRxqu5JCMWqZiN7NZxd0EiNlZXmHxhg0J5k1AmxvDE6UPr92xVtpjLz1FySqQjaZ7f+3y6Y933ejiUUseEJoAbqDVcVoJuW/WGRzIa4kxvmN7cBt6Fi5j8ZUIJm/DJEUIT56B7Yv/GrWsVd2rsLO3S2K0TA1KpMCFLsCzBGvD74laqBTYXn8N16n79nhPjTev35Gt55vJz+/13z/WdYyAxcP2LKqXUDWgCaKJQdVgMlnF6HvSlIoxFXFKbKzS++U3c4gbh7gThF5zCPnEmuHGreaG1yvw9oLwAABBNSURBVFaVnbk8Tq5GyILenhixvpjfCD3oi9uo19lYnKWU3yESTzB88hSxZOq6Imq/+utlXvq9fv/diBXZ779riXbcUkodnSaAgDGGraI/v79TqmNbwmh3nFFTJbT8NO7ys7hV/8Ju+OyLsYZPQ3Lgugu7e4XWatsV8otFaoU6dsiiazRJajRJKBPdX7ljjCG/sU52Zcmv33NinO7BYcSyriqi9vAravzDf87z23+4xluNxY+9boqx9BghS399Sqnb1/HF4K7tthUNW4x1xRh2CnhzT+OtzSJukchQL+GZs36phvjV8+zGM3hFv9Cak69RylapFBxIhEiPpeiaSGOHrz5DqJXLbC7OUSsViae7GJiYIhy7smLn/Hl47AMOMy9ZYrmwDMDSUyP87jsnefLrzaeZlFIKtBjcTVUdl6Ws322r4Rq6EmHO98bo3t3GfeLLNLaXsKw6sdEhQjMv92vwh6/0zjWuh1t08Hb9QmvG9SgXHUo1DxO1SZ3P0DOcxA5fPT3jeS47q5fJrV2p35Puvbokg2c8nttaIXxykcVdh8HEIFOZKV4xlOAtN6igqXX3lVJH0XEJIFeus5StsFHwyzQMpmOMJ23i22s4X/oGTm4FO24TnxnFnn4B0j0BdnA3ruP55ZQL9f1Ca4SEmkCh5tGIhUgMRekdSRKOXn9NoLybZ2txHqdWJd03QN/YOHboyg1a+/13d+d46DuqXLrYzRsfOUk6kgb8MsqHFVHTuvtKqaPqiARwXbctW5jsSzAaBWtpHuf/nqReXCeUSRB5wRnsiXOQGgbLwqu7eDsV3GIdr9wArhRaqwvsZKs4NY9oJsLAyJXSDQe5DYft5SUK25uEozFGTp0hkbm6Ame2muVS7hIlp0QynOSXHn0h7/m5XqZvsYjae9/r//Hfq7r5qlf52+94hyYApVRzxzoBOK7Hyk6Fpb1uWxGb08NpBk0Vb+EijfmLuJUs4f4MkbMvxRo9C8k+vGoDd6uKW6hjai4AVixEaDCBnY7Q8AzbqyUqRYdQxGJwIk2yu3kTlcL2FttLC7iuS/fwKD0jo1gHVgwV6gUu5S6Rq+WIhWKc7T3LYGKQh98sJOXWi6hp3X2l1FEdywRQa7jMbpZYy1f9Mg3JCKeH4vRUd3Eufon65eeQRpHIUB+Rh78bBk5ivDhOoY57eQfjeCBgJcKEuqPYmQgStmnUXbaD0g2WLfSOJMn0xRDr+sqaTrXK5uI8lUKeaDLF6OQ0kfiVhitlp8zc7hyb5U3CVpiT3Sc5kTpx1ZLOoxRR07r7SqmjOpYJwBJho1BjKBNjrDtKPLtJ/ctfpLq1gCUO0ZEhQg+9EhOfwCmDt+hgGrtgCXYyjDUQxk5F9pdruq7H7lppvzZ/V3+crqE4dpOaPMbzyG2ssbO6giD0j0+RGRjcL79cd+ss7C5wuXgZEWEiM8F4evyOi7Vp3X2l1FEdywQQti2+c7ILd2WJ+n9eoJpbwY5aRKfGsYbP41n91HZd2KmBLdipCHY6jJWMIPaVd/PGO1Cb3zWkuqP0DCcIRZqXVK6WimwuzFGvlEl29dA/MbVfv6fhNVguLLNUWMIzHsPJu9t/V+vuK6WO6o4TgIjYwFeAFWPM60RkGngc6AX+D/hxY0xdRKLAXwEvAbaBR40x83f6+s14hTyVz30aU9jATiawp88i3adwTRq3BBLy/Br6mQhWItx0CqeUr7GzWsKpe8RTYXpGkkTjzYfLc12yK0vkN9exwxGGTz5vv36PZzxWS6ss5Beoe3UG4gNMd03fk/67WndfKXUUd+MM4BeBi8BeW6v3Ae83xjwuIn8KvA34YPB5xxjzkIi8KTju0bvw+teRSBTLspHJhyE9jhdOIbZFKBPBSkWwEqFDO2JVSw47qyWq5QaRqM3QVIZE5vAbr0q5HTYX5/36PQNDV9Xv2ShvMJefo9LY67977p7231VKqaO4owQgImPAa4H3Ar8s/l/VR4A3B4d8FPgt/ATwhuAxwCeBPxERMffiVmQ7gkw/gsT8d/l22u+LeyNOzWVnrUQpX8cOCf1jKVI90UMTRaNeZ2tpgVIuSyQWZ3jmHLGUXwE0V81xKX+JQr1AMpy8b/13lVLqKO70DOAx4J1AOtjuA3LGmEawvQycCB6fAJYAjDENEckHx28dfEIReTvwdoCJiYnbCkpCFrHn9V/VIvEwbsMjt1GmsO3fGNY9GKdrIN606UoQO4WtTbaXFzHm6vo9xXqR2fws2WqWiB3hTO8ZhhJD963/rlJKHcVtJwAReR2wYYz5qoh8997uJoeaW/jalR3GfAj4EPi1gG47vpv88TeeIb9VIb9ZwbiGVG+U7qEEofDhPXPrlTKbC/NUSwXi6QwDE9OEYzGqjSpzO3Osl9cJWSFOdp1kNDXakv67Sil1q+7kDOCVwOtF5DVADP8awGNAt4iEgrOAMeBycPwyMA4si0gI6AKyd/D6t8UYQynnN19vOB6JdISekQSRG0wRGc8ju7pCfn0Ny7IYnDpJuq8fx3W4lLu03393Ij3BeObOl3QqpdT9cNsJwBjzbuDdAMEZwK8YY35URP4O+BH8lUBvBf4x+JZ/Crb/O/j6Z+/J/P8NVIp1dlZL1Cou0bhN/3iGeOrGlTUrhV02F+ZwalVSvf30j0+AZbG4u8hiYRHXcxlKDjGVmdL+u0qpB8q9uA/gXcDjIvIe4GvAR4L9HwE+JiLP4b/zf9M9eO2m6tUG2dUSlYJDKGwxMJ4m2R254dy822iwvbxIYXuTUCTKyKkzxNMZ1kprzO3OUXfr9MX6mOmeIRlO3q8fRSml7pq7kgCMMZ8HPh88ngVe1uSYKvDGu/F6t6rhuOTWyxSzNcQWeoYTdPXHm677P6iQ3WJ7MajfMzRKz+go2doOT61/hZJTIhPJaP9dpdQD71jeCey5HvnNyn7phnR/jO7BBHboxheGnVqVzYWgfk8iycjkDFXb4etb3yBfy2v/XaXUsXI8E4BnyG9WSGQi9Aw3r81/kDGG/Poa2dXloH7PJKHuNM/uzrJV2SJiRXhez/MYSY7okk6l1LFxLBNAKGwzdqbnhks699TKJTbn56hVSiS7ekifGGa5ssra+tNYYjHdNc1YakyXdCqljp1jmQCAm/7x91yX7OVl8hvr2OEwfVNTZEMlntm+AMCJ1AkmM5OEbV3SqZQ6no5tAriRUm6HrcUFGk6NdP8AlYzFk+VnaXgNBhODTHdNEw/Fb/5ESin1AOuoBNBwHLaXFijubBOOxgmN9XHJXadWrNET62Gma2a//65SSh13HZEAjDEUtjfZXl7CeB69o2PMyzr5+jKpcIrTvafpjfW2OkyllLqvjn0CqFcrbM7PUS0ViKUyDE769XtqJZsTMsZAfEBX9iilOtKxTQDG89hZu0xubRWxLAYmZ8j0X1m/P5wcbmF0SinVescyATi1KqvPPoNTq5Dq6aNvfJJQWFfzKKXUQccyAYTCEcKxKP0TkyQy2oFLKaWaOZYJQCyLkYdOtzoMpZRqazdvmaWUUupY0gSglFIdShOAUkp1KE0ASinVoTQBKKVUh9IEoJRSHUoTgFJKdShNAEop1aHEGNPqGA4lIpvAwl18yn5g6y4+373yIMT5IMQIGufd9CDECBonwKQx5qbNy9s6AdxtIvIVY8xLWx3HzTwIcT4IMYLGeTc9CDGCxnkUOgWklFIdShOAUkp1qE5LAB9qdQC36EGI80GIETTOu+lBiBE0zlvWUdcAlFJKXdFpZwBKKaUCxzIBiEhMRP5XRL4uIk+JyG8H+6dF5Esi8qyI/I2IRNo0zr8UkTkRuRB8vKiVcQYx2SLyNRH5dLDdVmO5p0mc7TiW8yLyRBDPV4J9vSLymWA8PyMiPW0a52+JyMqB8XxNi2PsFpFPisi3ROSiiLyiTceyWZwtH8tjmQCAGvCIMebbgBcB3yciLwfeB7zfGHMK2AHe1sIY4fA4AX7VGPOi4ONC60Lc94vAxQPb7TaWe66NE9pvLAFeFcSztwzw14D/CMbzP4LtdnBtnOD/3vfG819aFpnvj4B/NcacAb4N/3ffjmPZLE5o8VgeywRgfMVgMxx8GOAR4JPB/o8CP9iC8PbdIM62IiJjwGuBDwfbQpuNJVwf5wPmDfjjCG0ynu1ORDLAdwEfATDG1I0xOdpsLG8QZ8sdywQA+1MBF4AN4DPAJSBnjGkEhywDJ1oV355r4zTGfCn40ntF5Bsi8n4RibYwRIDHgHcCXrDdRxuOJdfHuaedxhL8JP/vIvJVEXl7sG/IGLMKEHwebFl0VzSLE+AXgvH88xZPr8wAm8BfBNN+HxaRJO03lofFCS0ey2ObAIwxrjHmRcAY8DLgbLPD7m9UTQK4Jk4ROQ+8GzgDPAz0Au9qVXwi8jpgwxjz1YO7mxza0rE8JE5oo7E84JXGmG8Hvh/4eRH5rlYHdIhmcX4QOIk/ZbkK/EEL4wsB3w580BjzYqBEe0z3XOuwOFs+lsc2AewJTrU+D7wc6BaRUPClMeByq+K61oE4v88YsxpMD9WAv8BPYK3ySuD1IjIPPI4/9fMY7TeW18UpIh9vs7EEwBhzOfi8AXwKP6Z1ERkBCD5vtC5CX7M4jTHrwZsWD/gzWjuey8DygbPmT+L/oW23sWwaZzuM5bFMACIyICLdweM48Gr8iy6fA34kOOytwD+2JkLfIXF+68A/XsGfv3yyVTEaY95tjBkzxkwBbwI+a4z5UdpsLA+J88faaSyDOJIikt57DHxvENM/4Y8jtMF4Hhbn3ngGfojW/ttcA5ZE5HSw63uAb9JmY3lYnO0wlqGbH/JAGgE+KiI2fpL7W2PMp0Xkm8DjIvIe4GsEF2Va6LA4PysiA/hTLReAn2llkId4F+01lof5RJuN5RDwKT8fEQL+2hjzryLyZeBvReRtwCLwxhbGCIfH+bFgKa0B5oGfbl2IALwD/3ccAWaBnyT4v9RGYwnN4/xAq8dS7wRWSqkOdSyngJRSSt2cJgCllOpQmgCUUqpDaQJQSqkOpQlAKaU6lCYApZTqUJoAlFKqQ2kCUCogIlNBvfYPi8iTIvIJEXm1iHwhqC3/suDji0FRry/u3d0pIufE7+1wISjudSq4m/afxe/38KSIPNrqn1Gpg/RGMKUCIjIFPAe8GHgK+DLwdfxeB6/Hv3vzLUDZGNMQkVcDP2uM+WER+WPgf4wxe3d72sBr8Gs7/VTw/F3GmPx9/rGUOtRxLQWh1O2aM8Y8ASAiT+E3FjEi8gQwBXThl+84hX8Lfzj4vv8GfiPoSfD3xphng+/5fRF5H/BpY8x/3e8fRqkb0Skgpa5WO/DYO7Dt4b9h+h3gc8aY88APADEAY8xf458lVIB/E5FHjDHPAC8BngB+V0R+8/78CErdGj0DUOpouoCV4PFP7O0UkRlg1hjzgeDxC0XkW0DWGPNxESkePF6pdqBnAEodze/hv5v/Av48/55H8cslX8BvQPNXwAuA/w32/QbwnvsdrFI3oheBlVKqQ+kZgFJKdShNAEop1aE0ASilVIfSBKCUUh1KE4BSSnUoTQBKKdWhNAEopVSH0gSglFId6v8BPNDiG/ltIRAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code 6.8\n",
    "ax = d.plot.line(x='mass', y='brain', style=['ob'], fillstyle='none', mew=1)\n",
    "\n",
    "for i in range(d.index.size):\n",
    "    d_new = d.drop(d.index[i])\n",
    "\n",
    "    m_0 = smf.ols('brain ~ mass', data=d_new).fit()\n",
    "\n",
    "    x_pred = np.linspace(d_new.mass.min() -5, d_new.mass.max() + 5, 50)\n",
    "    y_pred = m_0.params.Intercept + m_0.params.mass * x_pred\n",
    "\n",
    "    ax.plot(x_pred, y_pred, alpha=.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rethinking: bias and variance.** underfitting/overfitting also caused bias problems / variance problems. confusing nomenclature as these terms heavily overloaded in different contexts. also, increasing bias can improve predictions for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. information theory and model performance\n",
    "\n",
    "Define what model needs to do well at: target. Proposed here: out-of-sample deviance\n",
    "\n",
    "### 6.2.1. firing the weatherperson\n",
    "\n",
    "Define target needs at least:\n",
    "\n",
    "* cost-benefit. cost of wrong prediction, value of correct prediction\n",
    "* performance in context. how good could a model be for this task?\n",
    "\n",
    "#### 6.1.1.1. costs and benefits\n",
    "\n",
    "different decision about better model for different cost/benefit analysis\n",
    "\n",
    "#### 6.1.1.2. measuring accuracy\n",
    "\n",
    "even if ignoring cost/benefit unclear which measure of accuracy to use. \n",
    "\n",
    "McElreath: joint probability is correct measure. reason: currectly counts up relative nr of ways each event could happen = likelihood in B's theorem. **revisit likelihood, model definition, model structure of earlier chapters**\n",
    "\n",
    "\"maximizing joint probability will identify right model\" **what does right model mean?**\n",
    "\n",
    "how to measure performance? should account for difficulty of task. ex: if adding more categories, task becomes harder.\n",
    "\n",
    "**rethinking: what is a true model?** real world = deterministic. if all info was known, predictions would not be probabilities but just 1/0 for rain vs sunshine. but: we are ignorant of some information. 'right' = correct probabilities given our state of ignorance. \"probability is in the model, not in the world\". **but then, is each model a true model given its own assumptions of ignorance? unclear, discuss.**\n",
    "\n",
    "### 6.2.2. information theory and uncertainty\n",
    "\n",
    "information = reduction in uncertainty through learning an outcome\n",
    "\n",
    "define: f(probability of rain) -> uncertainty\n",
    "\n",
    "measure of uncertainty should be \n",
    "\n",
    "* continuous\n",
    "* increasing for more events\n",
    "* additive\n",
    "\n",
    "exactly one f() satisfying all criteria: information entropy.\n",
    "\n",
    "$$\n",
    "H(p) = -E \\log (p_i) = - \\sum_{i=1}^n p_i \\log (p_i)\n",
    "$$\n",
    "\n",
    "read: \"uncertainty in a probability distribution is the average log-probability of an event\"\n",
    "\n",
    "McElreath does not explain derivation, but points out: all parts of definition are there for a reason, to satisfy requirements listed above. However, above all, information entropy being used b/c it's proven to be very productive and useful.\n",
    "\n",
    "ex. on page illustrates: entropy of weather for Abu Dhabi much lower, as weather very uniform, thus low uncertainty. If adding 'snow' as weather condition: entropy/uncertainty increases.\n",
    "\n",
    "code for calculating entropy for one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108643020548935"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Code 6.9\n",
    "\n",
    "p = np.array([0.3, 0.7])\n",
    "- np.sum((p * np.log(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overthinking: more on entropy.** \n",
    "* \"-1\" in definition above customary but not necessary. leads to entropy increase from zero instead of decrease from one. \n",
    "* base of logarithm arbitrary, as long as same based used for all comparisons\n",
    "* how to deal with 0-probabilty events? drop out. assume that $0 \\log(0) = 0$. \"not a trick but not obvious\". remember: no sense to keep event that never happens in model.\n",
    "\n",
    "**rethinking: the benefits of maximizing uncertainty** maximum entropy (maxent) = family of techniques for finding prob. distr. most consistent w/ current knowledge. given current knowledge, what distr. is least surprising? will be used to build GLMs later.\n",
    "\n",
    "### 6.2.3. from entropy to accuracy\n",
    "\n",
    "**discuss: entropy is way of quantifying uncertainty. but why is that necessary? already had probabilities around parameters to quantifiy uncertainty?**\n",
    "\n",
    "How to use info entropy to measure model perf in distance from optimum? Divergence!\n",
    "\n",
    "Divergence = \"the additional uncertainty induced by using probabilities from one distribution to describe another distribution\" = Kullback-Leibler divergence = K-L divergence\n",
    "\n",
    "divergence(p,q) = KL(p,q) = \"average difference in log proability between target p and model q\"\n",
    "\n",
    "difference of entropies: \"entropy of target p and cross entropy of using q to predict p\"\n",
    "\n",
    "true probabilities = p\n",
    "approximation = q\n",
    "\n",
    "KL(p,q) = 0 iff p = q\n",
    "KL(p,q) > 0 otherwise\n",
    "\n",
    "grows the more q is different from p\n",
    "\n",
    "**overthinking: cross entropy and divergence.** cross entropy: events arise according to p, but are expected according to q, inflating uncertainty. \n",
    "\n",
    "$$H(p,q) = - \\sum_i p_i \\log(q_i)$$ \n",
    "\n",
    "Divergence = additional uncertainty due to cross entropy, thus \n",
    "\n",
    "$$KL(p,q) = H(p,q) - H(p)$$\n",
    "\n",
    "**rethinking: divergence depends upon direction.** my intuition: high entropy distribution = lots of ignorance = don't know what to expect, no event much more likely than others. use this to predict other distribution means we won't be super surprised by any outcome. \n",
    "\n",
    "flip side: low entropy distribution = lots of expectations = some events are much more likely than others. if we use low entropy distribution to predict other distribution and they don't match well: very surprised by outcome, high divergence. outcome has high information value?\n",
    "\n",
    "### 6.2.4. from divergence to deviance\n",
    "Know how to measure distance from true probability distr. $p$. But how to use in practice when $p$ is unknown? (wouldn't need to find model otherwise)\n",
    "\n",
    "Solution: don't know distance from $p$, but can know whether $q$ or $r$ is relatively closer. Deviance as relative measure.\n",
    "\n",
    "Summing log probabilities for each observed case gives approx of $E\\log(q_i)$\n",
    "\n",
    "Don't need to know $p$ in expectation term \"because nature takes care of presenting the events to us\". **what does that mean?**\n",
    "\n",
    "absolute magnitude of deviance not meaningful, only difference between models is\n",
    "\n",
    "deviance = \n",
    "\n",
    "$$\n",
    "D(q) = -2 \\sum_i \\log(q_i)\n",
    "$$\n",
    "\n",
    "$i$ = case index, $q_i$ = likelihood of case $i$. divergence scales w/ sample size.\n",
    "\n",
    "calc deviance by computing log-prop of observed data given MAP param estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.92498968588757"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code 6.10\n",
    "\n",
    "m_6_1 = smf.ols('brain ~ mass', data=d).fit()\n",
    "\n",
    "-2 * m_6_1.llf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncertainty about params -> uncertainty about deviance. for fixed set of params deviance also fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overthinking: computing deviance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [sigma, b, a]\n",
      "Sampling 2 chains: 100%|██████████| 14000/14000 [00:10<00:00, 1309.07draws/s]\n"
     ]
    }
   ],
   "source": [
    "# Code 6.11\n",
    "\n",
    "d['mass_s'] = (d.mass - d.mass.mean()) / d.mass.std()\n",
    "\n",
    "with pm.Model() as m_6_8 :\n",
    "    a = pm.Normal('a', mu=np.mean(d['brain']), sd=10)\n",
    "    b = pm.Normal('b', mu=0, sd=10)\n",
    "    \n",
    "    # why '*10'?\n",
    "    sigma = pm.Uniform('sigma', 0, np.std(d['brain']) * 10)\n",
    "    \n",
    "    mu = pm.Deterministic('mu', a + b * d['mass_s'])\n",
    "    \n",
    "    brain = pm.Normal('brain', mu = mu, sd = sigma, observed = d['brain'])\n",
    "    \n",
    "    m_6_8 = pm.sample(2000, tune=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a        713.748269\n",
       "b          1.218698\n",
       "sigma    369.997387\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = pm.summary(m_6_8).loc[['a', 'b', 'sigma'], 'mean']\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.18132665042522"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = -2 * np.sum(stats.norm.logpdf(\n",
    "    x = d['brain'], \n",
    "    loc = theta['a'] + theta['b'] * d['mass_s'], \n",
    "    scale = theta['sigma']\n",
    "))\n",
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5. from deviance to out-of-sample\n",
    "\n",
    "Train deviance always improves for more complex model. Care about test (out-of-sample) deviance.\n",
    "\n",
    "On average: test deviance will be worse than train deviance. However, can be reversed for an given train/test set. Any given training sample may be very misleading. Any given test sample might be highly non-representative. Don't test one sample too much. No guarantees in statistical inference.\n",
    "\n",
    "\"True\" model might behave worse than other models due to those effects.\n",
    "\n",
    "Conclusion: deviance on train data improves with more complex models. Deviance on test data might or might not, depending on actual data generation process and amount of train data available.\n",
    "\n",
    "**Overthinking: simulated training and testing.** all R stuff, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.12\n",
    "# Code 6.13\n",
    "# Code 6.14\n",
    "\n",
    "# R stuff, does not seem useful to replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 regularization\n",
    "\n",
    "root cause of overfitting: model trusting training data too much. flat prior = all param values equally likely = model makes posterior that fits training data (through likelihood function) as much as possible\n",
    "\n",
    "one solution: regularizing prior, slowing rate of learning from train data.\n",
    "\n",
    "reg. prior requires tuning. too much reg. means model doesn't learn from useful features. too little means it learns from noise features.\n",
    "\n",
    "no prior is optimal, \"doing better is all we can hope for\"\n",
    "\n",
    "example: use prior of Normal(0, 1) for model coefficient, encodes belief that values greater than 2 or smaller than -2 are pretty unlikely. make more skeptical by giving smaller std dev, e.g. Normal(0, 0.5). \n",
    "\n",
    "how strong? depends on data and model.\n",
    "\n",
    "the more data, the smaller effect of prior\n",
    "\n",
    "how to pick strenght of regularization? if enough data: split into train/test and use that to tune, picking settings with lowest test deviance. essentially cross validation.\n",
    "\n",
    "if not enough data available to split into train/test: information criteria (next chapter)\n",
    "\n",
    "**rethinking: multilevel-models as adaptive regularization.** multilevel models learn strength of priors from data. can be seen as adaptive regularization.\n",
    "\n",
    "**rethinking: rdige regression.** lin. models with Gaussian priors having 0 mean also called ridge regression. typically takes param to tune strenght of regularization. ridge regression does not compute posteriors, but includes reg. parameter into closed-form algebra solution for least squares approach. however, can also be seen as bayesian. another example of seeing one method from two perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. information criteria\n",
    "\n",
    "information criteria: provide estimate of out-of-sample deviance for model.\n",
    "\n",
    "best-known criterion: akaike information criterion (aic):\n",
    "\n",
    "$$\n",
    "\\text{AIC} = D_{train} + 2p\n",
    "$$\n",
    "\n",
    "where $p$ = number of free model params\n",
    "\n",
    "All info criteria try to predict out-of-sample deviance for models but derived under different constraints and assumptions. For AIC these are:\n",
    "\n",
    "* flat priors or dominated by llhood\n",
    "* posterior is approx. multivar Normal\n",
    "* nr of data points much greater than nr of params\n",
    "\n",
    "'flat priors' harsh restriction, want to regularize. thus look for other criteria.\n",
    "\n",
    "deviance information criterion (dic) and widely applicable information criterion (waic)\n",
    "\n",
    "**rethinking: information criteria and consistency.** aic, dic, waic not consistent: do not necessarily assign best score to 'true' model. instead assign highest score to model with best expected out-of-sample predictive performance. issue? no, if goal is prediction. further \n",
    "\n",
    "furthermore, consistency defined for case dataset size going towards infinite, and in those info criteria give highest scores to most complex models. but in those cases v. complex behave like true model due to good fit.\n",
    "\n",
    "### 6.4.1. dic\n",
    "\n",
    "Aware of informative priors, but still assumes multivar Gaussian posterior. Thus results can be very wrong if that assumption not met.\n",
    "\n",
    "Calculated from posterior of deviance, which is based on posterior uncertainty around parameters.\n",
    "\n",
    "Calc deviance for each posterior sample. Let $\\bar D$ be the average deviance and $\\hat D$ the deviance at the posterior mean, then\n",
    "\n",
    "$$\n",
    "\\text{DIC} = \\bar D + (\\bar D - \\hat D) = \\bar D + p_D\n",
    "$$\n",
    "\n",
    "$p_D$ encodes model complexity and penalizes DIC based on that. = expected distance between in-sample and out-of-sample deviance. for flat priors: dic = aic, as expected difference = nr of parameters. however, for informative priors, model more constrained, thus penalty fraction of nr of parameters. \n",
    "\n",
    "**discuss: why is $\\bar D - \\hat D$ = expected diff between in-sample and out-of-sample deviance?**\n",
    "\n",
    "### 6.4.2. waic\n",
    "\n",
    "essential characteristic: pointwise. considers uncertainty around each prediction individually. makes sense b/c some predictions are harder/more uncertain than others. aic/dic don't do this.\n",
    "\n",
    "$$\n",
    "Pr(y_i) = \\text{avg}_{\\text{posterior samples}}(P(y_i | \\text{post. samp.})\n",
    "$$\n",
    "\n",
    "thus: average likelihood of $y_i$ over posterior\n",
    "\n",
    "$$\n",
    "lppd = \\sum_{i=1}^N \\log Pr(y_i)\n",
    "$$\n",
    "\n",
    "lppd = log pointwise predictive density = sum of log $Pr(y_i)$ across all data points = pointwise analog of deviance over posterior.\n",
    "\n",
    "$$\n",
    "V(y_i) = \\text{variance of log-likelihood for sample i in train data}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "p_{waic} = \\sum_i^N V(y_i) = \\text{effective nr of parameters}\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "waic = -2(\\text{lppd} - p_{waic})\n",
    "$$\n",
    "\n",
    "again, estimating out of sample deviance.\n",
    "\n",
    "issue: requires individual consideration of each datum. not meaningful for time series, where each datum depends on the one before. generally, assumes exchangeability.\n",
    "\n",
    "generally: validity of any information criterion depends on predictive task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rethinking: what about bic?** bayesian info crit. (bic), or Schwarz criterion. not bayes/non-bayes choice to decide for/against this. can be motivaed from bayes/non-bayes perspective. bic related to lhood of data averaged over all priors = evidence in bayes formula -> comparing two models on evidence = bayes factor. not treated in this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**overthinking: waic calculations.** see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6.15\n",
    "# Code 6.16\n",
    "# Code 6.17\n",
    "# Code 6.18\n",
    "# Code 6.19\n",
    "# Code 6.20\n",
    "\n",
    "# not super relevant at the moment, skip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3. dic and waic as estimates of deviance\n",
    "\n",
    "both accurate in given example. help to detect overfitting. complementery with regularization, which helps to reduce overfitting but does not detect/measure it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rethinking: diverse prediction frameworks.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
